{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11836250,"sourceType":"datasetVersion","datasetId":7436238}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predicting Lung Cancer and Survey Analysis","metadata":{}},{"cell_type":"markdown","source":"## Summary/ Introduction\n\nThis notebook analyzes survey responses to identify behavioral and demographic factors linked to lung cancer. Using permutation-based feature importance, we find that only 5 out of 15 survey questions are needed to accurately predict the presence of lung cancer. We also demonstrate how to adjust the model to prioritize minimizing false negatives by using the F-beta score (with beta > 1), which emphasizes recall. This trade-off is critical in medical contexts, where failing to identify at-risk individuals can delay diagnosis and treatment. By surfacing high-risk patterns in survey responses, this approach offers a potential tool for early detection and clinical follow-up.","metadata":{}},{"cell_type":"markdown","source":"## Data Overview\n\nThe [Lung Cancer Dataset](https://www.kaggle.com/datasets/aagambshah/lung-cancer-dataset) contains **309 survey responses** capturing demographic and behavioral factors. The target variable is `LUNG_CANCER`, indicating whether a respondent has been diagnosed with lung cancer. Below is a description of each feature:\n\n| Feature Name            | Description                                      |\n| ----------------------- | ------------------------------------------------ |\n| `GENDER`                | Gender of the respondent (Male/Female)           |\n| `AGE`                   | Age of the respondent                            |\n| `SMOKING`               | Smoking habit (Yes/No)                           |\n| `YELLOW_FINGERS`        | Presence of yellowing fingers (Yes/No)           |\n| `ANXIETY`               | Presence of anxiety (Yes/No)                     |\n| `PEER_PRESSURE`         | Experience of peer pressure (Yes/No)             |\n| `CHRONIC DISEASE`       | Existing chronic diseases (Yes/No)               |\n| `FATIGUE`               | Presence of fatigue (Yes/No)                     |\n| `ALLERGY`               | Allergic conditions (Yes/No)                     |\n| `WHEEZING`              | Wheezing symptoms (Yes/No)                       |\n| `ALCOHOL CONSUMING`     | Alcohol consumption habit (Yes/No)               |\n| `COUGHING`              | Frequent coughing (Yes/No)                       |\n| `SHORTNESS OF BREATH`   | Symptom of shortness of breath (Yes/No)          |\n| `SWALLOWING DIFFICULTY` | Difficulty in swallowing (Yes/No)                |\n| `CHEST PAIN`            | Presence of chest pain (Yes/No)                  |\n| `LUNG_CANCER`           | Lung cancer diagnosis (target variable) (Yes/No) |\n","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"# Core libraries\nimport os\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Kaggle dataset access\nimport kagglehub\n\n# Scikit-learn: modeling and evaluation\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.inspection import permutation_importance\n\nfrom sklearn.metrics import (\n    accuracy_score,\n    fbeta_score,\n    make_scorer,\n    precision_score,\n    recall_score,\n    confusion_matrix,\n    ConfusionMatrixDisplay,\n    precision_recall_curve,\n    PrecisionRecallDisplay,\n    roc_curve,\n    RocCurveDisplay\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:43:31.629440Z","iopub.execute_input":"2025-06-19T19:43:31.630306Z","iopub.status.idle":"2025-06-19T19:43:36.891460Z","shell.execute_reply.started":"2025-06-19T19:43:31.630275Z","shell.execute_reply":"2025-06-19T19:43:36.890326Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Cleaning & Preprocessing","metadata":{}},{"cell_type":"code","source":"# Download latest version\npath = kagglehub.dataset_download(\"aagambshah/lung-cancer-dataset\")\n\n# Load dataset into DataFrame\ndf = pd.read_csv(os.path.join(path, 'survey lung cancer.csv'))\n\n# Preview\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:43:36.893048Z","iopub.execute_input":"2025-06-19T19:43:36.893638Z","iopub.status.idle":"2025-06-19T19:43:37.096188Z","shell.execute_reply.started":"2025-06-19T19:43:36.893611Z","shell.execute_reply":"2025-06-19T19:43:37.095311Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for nulls\ndf.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:43:37.097104Z","iopub.execute_input":"2025-06-19T19:43:37.097372Z","iopub.status.idle":"2025-06-19T19:43:37.105818Z","shell.execute_reply.started":"2025-06-19T19:43:37.097345Z","shell.execute_reply":"2025-06-19T19:43:37.104820Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check data types\ndf.dtypes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:43:37.107524Z","iopub.execute_input":"2025-06-19T19:43:37.107783Z","iopub.status.idle":"2025-06-19T19:43:37.128064Z","shell.execute_reply.started":"2025-06-19T19:43:37.107763Z","shell.execute_reply":"2025-06-19T19:43:37.127148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# View unique values\nfor col in df.select_dtypes(include=['object', 'int64']):\n    print(f'{col}: {df[col].unique()}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:43:37.129111Z","iopub.execute_input":"2025-06-19T19:43:37.129393Z","iopub.status.idle":"2025-06-19T19:43:37.156908Z","shell.execute_reply.started":"2025-06-19T19:43:37.129371Z","shell.execute_reply":"2025-06-19T19:43:37.155697Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clean column names\ndf.columns = df.columns.str.strip().str.replace(' ', '_')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:43:37.158088Z","iopub.execute_input":"2025-06-19T19:43:37.158353Z","iopub.status.idle":"2025-06-19T19:43:37.176996Z","shell.execute_reply.started":"2025-06-19T19:43:37.158331Z","shell.execute_reply":"2025-06-19T19:43:37.175821Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Helper function\ndef one_hot_encode(array, positive_class):\n    \"\"\"\n    Encodes a binary categorical array into 0s and 1s based on a specified positive class.\n\n    Parameters:\n        array (array-like): Input array containing binary values (e.g., 'Yes'/'No', 1/2).\n        positive_class (str or int): The class to encode as 1. All other values become 0.\n\n    Returns:\n        np.ndarray: Array of 0s and 1s, where 1 represents the positive_class.\n    \"\"\"\n    return np.where(array == positive_class, 1, 0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:43:37.178044Z","iopub.execute_input":"2025-06-19T19:43:37.178350Z","iopub.status.idle":"2025-06-19T19:43:37.195008Z","shell.execute_reply.started":"2025-06-19T19:43:37.178320Z","shell.execute_reply":"2025-06-19T19:43:37.193892Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# One-hot encode\ndf_encoded = df.copy()\ndf_encoded.GENDER = one_hot_encode(df.GENDER.values, 'M')\ndf_encoded.SMOKING = one_hot_encode(df.SMOKING.values, 2)\ndf_encoded.YELLOW_FINGERS = one_hot_encode(df.YELLOW_FINGERS.values, 2)\ndf_encoded.ANXIETY = one_hot_encode(df.ANXIETY.values, 2)\ndf_encoded.PEER_PRESSURE = one_hot_encode(df.PEER_PRESSURE.values, 2)\ndf_encoded.CHRONIC_DISEASE = one_hot_encode(df.CHRONIC_DISEASE.values, 2)\ndf_encoded.FATIGUE = one_hot_encode(df.FATIGUE.values, 2)\ndf_encoded.ALLERGY = one_hot_encode(df.ALLERGY.values, 2)\ndf_encoded.WHEEZING = one_hot_encode(df.WHEEZING.values, 2)\ndf_encoded.ALCOHOL_CONSUMING = one_hot_encode(df.ALCOHOL_CONSUMING.values, 2)\ndf_encoded.COUGHING = one_hot_encode(df.COUGHING.values, 2)\ndf_encoded.SHORTNESS_OF_BREATH = one_hot_encode(df.SHORTNESS_OF_BREATH.values, 2)\ndf_encoded.SWALLOWING_DIFFICULTY = one_hot_encode(df.SWALLOWING_DIFFICULTY.values, 2)\ndf_encoded.CHEST_PAIN = one_hot_encode(df.CHEST_PAIN.values, 2)\ndf_encoded.LUNG_CANCER = one_hot_encode(df.LUNG_CANCER.values, 'YES')\n\n# Preview\ndf_encoded.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:43:37.195991Z","iopub.execute_input":"2025-06-19T19:43:37.196236Z","iopub.status.idle":"2025-06-19T19:43:37.223831Z","shell.execute_reply.started":"2025-06-19T19:43:37.196217Z","shell.execute_reply":"2025-06-19T19:43:37.222811Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"code","source":"# Histograms of all features\nax = df_encoded.hist(figsize=(12, 10), edgecolor='black', grid=False)\nplt.suptitle(\"Distribution of Encoded Features\", fontsize=20)\nplt.tight_layout(rect=[0, 0.03, 1, 0.97])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:43:37.224832Z","iopub.execute_input":"2025-06-19T19:43:37.225156Z","iopub.status.idle":"2025-06-19T19:43:40.588730Z","shell.execute_reply.started":"2025-06-19T19:43:37.225122Z","shell.execute_reply":"2025-06-19T19:43:40.587754Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"By looking at `LUNG_CANCER`, it becomes obvious that this is an imbalanced dataset with many more cases of lung cancer than those without. Sampling methods will need to be used to prevent the model from becoming biased towards the majority class.","metadata":{}},{"cell_type":"code","source":"# Check for collinearity\ncorrelation_matrix = df_encoded.corr()\n\n# Visualize\nplt.figure(figsize=(12, 10))\nsns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', square=True)\nplt.title('Feature Correlation Matrix')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:43:40.591944Z","iopub.execute_input":"2025-06-19T19:43:40.592254Z","iopub.status.idle":"2025-06-19T19:43:41.425435Z","shell.execute_reply.started":"2025-06-19T19:43:40.592231Z","shell.execute_reply":"2025-06-19T19:43:41.424589Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Building","metadata":{}},{"cell_type":"markdown","source":"### Datasets","metadata":{}},{"cell_type":"code","source":"# Separate features from target\nX = df_encoded.drop('LUNG_CANCER', axis=1).values\ny = df_encoded['LUNG_CANCER'].values\nfeature_names = df_encoded.drop('LUNG_CANCER', axis=1).columns\n\n# Split X, y into train and test splits and stratify based on target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle=True, stratify=y)\n\n# Split X_train, y_train into train and validation splits and stratify based on y_train (for feature importance scoring)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.33, random_state=42, shuffle=True, stratify=y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:43:41.426449Z","iopub.execute_input":"2025-06-19T19:43:41.426808Z","iopub.status.idle":"2025-06-19T19:43:41.444990Z","shell.execute_reply.started":"2025-06-19T19:43:41.426772Z","shell.execute_reply":"2025-06-19T19:43:41.443954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Helper function\ndef plot_cv_results(results1, results2, x1='param_max_depth', x2='min_samples_leaf'):\n    # Create subplots\n    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), constrained_layout=True, sharey=True)\n\n    # Left plot\n    ax1.plot(results1[x1].astype('int'), results1['mean_train_score'], label='Train')\n    ax1.fill_between(results1[x1].astype('int'),\n                     [m + s for m, s in zip(results1['mean_train_score'], results1['std_train_score'])],\n                     [m - s for m, s in zip(results1['mean_train_score'], results1['std_train_score'])],\n                     alpha=0.3)\n    \n    ax1.plot(results1[x1].astype('int'), results1['mean_test_score'], label='Val')\n    ax1.fill_between(results1[x1].astype('int'),\n                     [m + s for m, s in zip(results1['mean_test_score'], results1['std_test_score'])],\n                     [m - s for m, s in zip(results1['mean_test_score'], results1['std_test_score'])],\n                     alpha=0.3)\n    \n    ax1.legend()\n    ax1.set_xlabel(x1)\n    ax1.set_ylabel('F-beta Score')\n    ax1.set_title('Maximum Tree Depth Approach')\n\n    # Right plot\n    ax2.plot(results2[x2].astype('int'), results2['mean_train_score'], label='Train')\n    ax2.fill_between(results2[x2].astype('int'),\n                     [m + s for m, s in zip(results2['mean_train_score'], results2['std_train_score'])],\n                     [m - s for m, s in zip(results2['mean_train_score'], results2['std_train_score'])],\n                     alpha=0.3)\n    \n    ax2.plot(results2[x2].astype('int'), results2['mean_test_score'], label='Val')\n    ax2.fill_between(results2[x2].astype('int'),\n                     [m + s for m, s in zip(results2['mean_test_score'], results2['std_test_score'])],\n                     [m - s for m, s in zip(results2['mean_test_score'], results2['std_test_score'])],\n                     alpha=0.3)\n    \n    ax2.legend()\n    ax2.set_xlabel(x2)\n    ax2.set_ylabel('F-beta Score')\n    ax2.set_title('Minimum Samples Per Leaf Approach')\n    \n    fig.suptitle('Cross-Validation F-beta Scores')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:43:41.446111Z","iopub.execute_input":"2025-06-19T19:43:41.446461Z","iopub.status.idle":"2025-06-19T19:43:41.470978Z","shell.execute_reply.started":"2025-06-19T19:43:41.446415Z","shell.execute_reply":"2025-06-19T19:43:41.469817Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Tree-Depth Approach vs Minimum Samples Per Leaf Approach","metadata":{}},{"cell_type":"code","source":"# Define parameter grid\nparams1 = {'max_depth': list(range(2, 11)),\n           'random_state': [42]}\nparams2 = {'min_samples_leaf': list(range(1, 11)),\n           'random_state': [42]}\n\n# Scoring with F-beta score\nfbeta_scorer = make_scorer(fbeta_score, beta=2)\n\n# Create Grid Search object\ngrid1 = GridSearchCV(\n    estimator=RandomForestClassifier(),\n    param_grid=params1,\n    scoring=fbeta_scorer,\n    cv=5,\n    return_train_score=True\n)\ngrid2 = GridSearchCV(\n    estimator=RandomForestClassifier(),\n    param_grid=params2,\n    scoring=fbeta_scorer,\n    cv=5,\n    return_train_score=True\n)\n\n# Train\ngrid1.fit(X_train, y_train)\ngrid2.fit(X_train, y_train)\n\n# Plot results\ncv_results1 = pd.DataFrame(grid1.cv_results_)[['param_max_depth', 'mean_train_score', 'std_train_score', 'mean_test_score', 'std_test_score']]\ncv_results2 = pd.DataFrame(grid2.cv_results_)[['param_min_samples_leaf', 'mean_train_score', 'std_train_score', 'mean_test_score', 'std_test_score']]\nplot_cv_results(cv_results1, cv_results2, x1='param_max_depth', x2='param_min_samples_leaf')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:43:41.472319Z","iopub.execute_input":"2025-06-19T19:43:41.472667Z","iopub.status.idle":"2025-06-19T19:43:56.216597Z","shell.execute_reply.started":"2025-06-19T19:43:41.472634Z","shell.execute_reply":"2025-06-19T19:43:56.215698Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Setting the maximum tree depth leads to overfitting as seen by the continued increase in model performance on the train dataset, but decreased performance on the validation dataset. Setting the minimum samples per leaf leads to better generalization which can be seen by the lack of overfitting, and low variance across cross validation runs.","metadata":{}},{"cell_type":"code","source":"# Verify we agree with GridSearchCV's best parameters\ngrid2.best_params_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:43:56.217669Z","iopub.execute_input":"2025-06-19T19:43:56.218018Z","iopub.status.idle":"2025-06-19T19:43:56.223823Z","shell.execute_reply.started":"2025-06-19T19:43:56.217989Z","shell.execute_reply":"2025-06-19T19:43:56.222907Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Importance (Permutation-Based)\nNow that we have a first pass at a successful model, let's measure permutation-based feature importance to decide which survey questions positively impact predictive performance. The model was trained on the train dataset and will now be tested on the validation dataset.","metadata":{}},{"cell_type":"code","source":"# Compute permutation importance\nresult = permutation_importance(grid2, X_val, y_val, n_repeats=30, random_state=42, scoring=fbeta_scorer)\n\n# Create a sorted Series for plotting\nimportances = pd.Series(result.importances_mean, index=feature_names)\nimportances_std = pd.Series(result.importances_std, index=feature_names)\nsorted_importances = importances.sort_values(ascending=False)\n\n# Plot with matching error bars\nfig, ax = plt.subplots(figsize=(10, 6))\nsorted_importances.plot.bar(yerr=importances_std[sorted_importances.index], ax=ax)\nax.set_title(\"Permutation Importance (Validation Set)\")\nax.set_ylabel(\"Mean Accuracy Decrease\")\nfig.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:43:56.224723Z","iopub.execute_input":"2025-06-19T19:43:56.225009Z","iopub.status.idle":"2025-06-19T19:43:59.456763Z","shell.execute_reply.started":"2025-06-19T19:43:56.224983Z","shell.execute_reply":"2025-06-19T19:43:59.455677Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"`AGE` and `CHRONIC_DISEASE` both show that the model performance increases when their values are permuted, meaning they are useless. Let's remove them and repeat the process.","metadata":{}},{"cell_type":"code","source":"# Drop features (datsets do not leak since random_state kept constant)\nX = df_encoded.drop(columns=['AGE', 'CHRONIC_DISEASE', 'LUNG_CANCER']).values\ny = df_encoded['LUNG_CANCER'].values\nfeature_names = df_encoded.drop(columns=['AGE', 'CHRONIC_DISEASE', 'LUNG_CANCER']).columns\n\n# Split X, y into train and test splits and stratify based on target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle=True, stratify=y)\n\n# Split X_train, y_train into train and validation splits and stratify based on y_train (for feature importance scoring)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.33, random_state=42, shuffle=True, stratify=y_train)\n\n# Train\ngrid1.fit(X_train, y_train)\ngrid2.fit(X_train, y_train)\n\n# Plot results\ncv_results1 = pd.DataFrame(grid1.cv_results_)[['param_max_depth', 'mean_train_score', 'std_train_score', 'mean_test_score', 'std_test_score']]\ncv_results2 = pd.DataFrame(grid2.cv_results_)[['param_min_samples_leaf', 'mean_train_score', 'std_train_score', 'mean_test_score', 'std_test_score']]\nplot_cv_results(cv_results1, cv_results2, x1='param_max_depth', x2='param_min_samples_leaf')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:43:59.458176Z","iopub.execute_input":"2025-06-19T19:43:59.458545Z","iopub.status.idle":"2025-06-19T19:44:14.274920Z","shell.execute_reply.started":"2025-06-19T19:43:59.458513Z","shell.execute_reply":"2025-06-19T19:44:14.273961Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verify we agree with GridSearchCV's best parameters\ngrid2.best_params_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:44:14.275926Z","iopub.execute_input":"2025-06-19T19:44:14.276234Z","iopub.status.idle":"2025-06-19T19:44:14.282128Z","shell.execute_reply.started":"2025-06-19T19:44:14.276204Z","shell.execute_reply":"2025-06-19T19:44:14.281293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute permutation importance\nresult = permutation_importance(grid2, X_val, y_val, n_repeats=30, random_state=42, scoring=fbeta_scorer)\n\n# Create a sorted Series for plotting\nimportances = pd.Series(result.importances_mean, index=feature_names)\nimportances_std = pd.Series(result.importances_std, index=feature_names)\nsorted_importances = importances.sort_values(ascending=False)\n\n# Plot with matching error bars\nfig, ax = plt.subplots(figsize=(10, 6))\nsorted_importances.plot.bar(yerr=importances_std[sorted_importances.index], ax=ax)\nax.set_title(\"Permutation Importance (Validation Set)\")\nax.set_ylabel(\"Mean Accuracy Decrease\")\nfig.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:44:14.283075Z","iopub.execute_input":"2025-06-19T19:44:14.283359Z","iopub.status.idle":"2025-06-19T19:44:17.176584Z","shell.execute_reply.started":"2025-06-19T19:44:14.283330Z","shell.execute_reply":"2025-06-19T19:44:17.175415Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, `GENDER`, `SMOKING`, and `FATIGUE` show that the model score is not affected by their shuffled values. Therefore, we should remove these features as well and repeat the process.","metadata":{}},{"cell_type":"code","source":"# Drop features (datsets do not leak since random_state kept constant)\nX = df_encoded.drop(columns=['GENDER', 'SMOKING', 'FATIGUE', 'AGE', 'CHRONIC_DISEASE', 'LUNG_CANCER']).values\ny = df_encoded['LUNG_CANCER'].values\nfeature_names = df_encoded.drop(columns=['GENDER', 'SMOKING', 'FATIGUE', 'AGE', 'CHRONIC_DISEASE', 'LUNG_CANCER']).columns\n\n# Split X, y into train and test splits and stratify based on target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle=True, stratify=y)\n\n# Split X_train, y_train into train and validation splits and stratify based on y_train (for feature importance scoring)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.33, random_state=42, shuffle=True, stratify=y_train)\n\n# Train\ngrid1.fit(X_train, y_train)\ngrid2.fit(X_train, y_train)\n\n# Plot results\ncv_results1 = pd.DataFrame(grid1.cv_results_)[['param_max_depth', 'mean_train_score', 'std_train_score', 'mean_test_score', 'std_test_score']]\ncv_results2 = pd.DataFrame(grid2.cv_results_)[['param_min_samples_leaf', 'mean_train_score', 'std_train_score', 'mean_test_score', 'std_test_score']]\nplot_cv_results(cv_results1, cv_results2, x1='param_max_depth', x2='param_min_samples_leaf')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:44:17.177887Z","iopub.execute_input":"2025-06-19T19:44:17.178747Z","iopub.status.idle":"2025-06-19T19:44:31.951731Z","shell.execute_reply.started":"2025-06-19T19:44:17.178713Z","shell.execute_reply":"2025-06-19T19:44:31.950727Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verify we agree with GridSearchCV's best parameters\ngrid2.best_params_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:44:31.952662Z","iopub.execute_input":"2025-06-19T19:44:31.952937Z","iopub.status.idle":"2025-06-19T19:44:31.959412Z","shell.execute_reply.started":"2025-06-19T19:44:31.952916Z","shell.execute_reply":"2025-06-19T19:44:31.958206Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute permutation importance\nresult = permutation_importance(grid2, X_val, y_val, n_repeats=30, random_state=42, scoring=fbeta_scorer)\n\n# Create a sorted Series for plotting\nimportances = pd.Series(result.importances_mean, index=feature_names)\nimportances_std = pd.Series(result.importances_std, index=feature_names)\nsorted_importances = importances.sort_values(ascending=False)\n\n# Plot with matching error bars\nfig, ax = plt.subplots(figsize=(10, 6))\nsorted_importances.plot.bar(yerr=importances_std[sorted_importances.index], ax=ax)\nax.set_title(\"Permutation Importance (Validation Set)\")\nax.set_ylabel(\"Mean Accuracy Decrease\")\nfig.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:44:31.960555Z","iopub.execute_input":"2025-06-19T19:44:31.960961Z","iopub.status.idle":"2025-06-19T19:44:34.116318Z","shell.execute_reply.started":"2025-06-19T19:44:31.960930Z","shell.execute_reply":"2025-06-19T19:44:34.115318Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, shuffling `ANXIETY` tends to make the model perform better. Therefore, this feature needs to be removed as well.","metadata":{}},{"cell_type":"code","source":"# Drop features (datsets do not leak since random_state kept constant)\nX = df_encoded.drop(columns=['ANXIETY', 'GENDER', 'SMOKING', 'FATIGUE', 'AGE', 'CHRONIC_DISEASE', 'LUNG_CANCER']).values\ny = df_encoded['LUNG_CANCER'].values\nfeature_names = df_encoded.drop(columns=['ANXIETY', 'GENDER', 'SMOKING', 'FATIGUE', 'AGE', 'CHRONIC_DISEASE', 'LUNG_CANCER']).columns\n\n# Split X, y into train and test splits and stratify based on target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle=True, stratify=y)\n\n# Split X_train, y_train into train and validation splits and stratify based on y_train (for feature importance scoring)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.33, random_state=42, shuffle=True, stratify=y_train)\n\n# Train\ngrid1.fit(X_train, y_train)\ngrid2.fit(X_train, y_train)\n\n# Plot results\ncv_results1 = pd.DataFrame(grid1.cv_results_)[['param_max_depth', 'mean_train_score', 'std_train_score', 'mean_test_score', 'std_test_score']]\ncv_results2 = pd.DataFrame(grid2.cv_results_)[['param_min_samples_leaf', 'mean_train_score', 'std_train_score', 'mean_test_score', 'std_test_score']]\nplot_cv_results(cv_results1, cv_results2, x1='param_max_depth', x2='param_min_samples_leaf')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:44:34.117577Z","iopub.execute_input":"2025-06-19T19:44:34.117924Z","iopub.status.idle":"2025-06-19T19:44:48.803292Z","shell.execute_reply.started":"2025-06-19T19:44:34.117895Z","shell.execute_reply":"2025-06-19T19:44:48.802070Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verify we agree with GridSearchCV's best parameters\ngrid2.best_params_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:44:48.804345Z","iopub.execute_input":"2025-06-19T19:44:48.804629Z","iopub.status.idle":"2025-06-19T19:44:48.811170Z","shell.execute_reply.started":"2025-06-19T19:44:48.804607Z","shell.execute_reply":"2025-06-19T19:44:48.810270Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute permutation importance\nresult = permutation_importance(grid2, X_val, y_val, n_repeats=30, random_state=42, scoring=fbeta_scorer)\n\n# Create a sorted Series for plotting\nimportances = pd.Series(result.importances_mean, index=feature_names)\nimportances_std = pd.Series(result.importances_std, index=feature_names)\nsorted_importances = importances.sort_values(ascending=False)\n\n# Plot with matching error bars\nfig, ax = plt.subplots(figsize=(10, 6))\nsorted_importances.plot.bar(yerr=importances_std[sorted_importances.index], ax=ax)\nax.set_title(\"Permutation Importance (Validation Set)\")\nax.set_ylabel(\"Mean Accuracy Decrease\")\nfig.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:44:48.812302Z","iopub.execute_input":"2025-06-19T19:44:48.812738Z","iopub.status.idle":"2025-06-19T19:44:50.919917Z","shell.execute_reply.started":"2025-06-19T19:44:48.812710Z","shell.execute_reply":"2025-06-19T19:44:50.918890Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Once again, a feature that is not affecting the model prediction has surfaced. Removing `SHORTNESS_OF_BREATH` and repeating.","metadata":{}},{"cell_type":"code","source":"# Drop features (datsets do not leak since random_state kept constant)\nX = df_encoded.drop(columns=['SHORTNESS_OF_BREATH', 'ANXIETY', 'GENDER', 'SMOKING', 'FATIGUE', 'AGE', 'CHRONIC_DISEASE', 'LUNG_CANCER']).values\ny = df_encoded['LUNG_CANCER'].values\nfeature_names = df_encoded.drop(columns=['SHORTNESS_OF_BREATH', 'ANXIETY', 'GENDER', 'SMOKING', 'FATIGUE', 'AGE', 'CHRONIC_DISEASE', 'LUNG_CANCER']).columns\n\n# Split X, y into train and test splits and stratify based on target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle=True, stratify=y)\n\n# Split X_train, y_train into train and validation splits and stratify based on y_train (for feature importance scoring)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.33, random_state=42, shuffle=True, stratify=y_train)\n\n# Train\ngrid1.fit(X_train, y_train)\ngrid2.fit(X_train, y_train)\n\n# Plot results\ncv_results1 = pd.DataFrame(grid1.cv_results_)[['param_max_depth', 'mean_train_score', 'std_train_score', 'mean_test_score', 'std_test_score']]\ncv_results2 = pd.DataFrame(grid2.cv_results_)[['param_min_samples_leaf', 'mean_train_score', 'std_train_score', 'mean_test_score', 'std_test_score']]\nplot_cv_results(cv_results1, cv_results2, x1='param_max_depth', x2='param_min_samples_leaf')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:44:50.921100Z","iopub.execute_input":"2025-06-19T19:44:50.921373Z","iopub.status.idle":"2025-06-19T19:45:05.634335Z","shell.execute_reply.started":"2025-06-19T19:44:50.921351Z","shell.execute_reply":"2025-06-19T19:45:05.633412Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verify we agree with GridSearchCV's best parameters\ngrid2.best_params_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:45:05.635429Z","iopub.execute_input":"2025-06-19T19:45:05.635673Z","iopub.status.idle":"2025-06-19T19:45:05.642160Z","shell.execute_reply.started":"2025-06-19T19:45:05.635654Z","shell.execute_reply":"2025-06-19T19:45:05.641133Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute permutation importance\nresult = permutation_importance(grid2, X_val, y_val, n_repeats=30, random_state=42, scoring=fbeta_scorer)\n\n# Create a sorted Series for plotting\nimportances = pd.Series(result.importances_mean, index=feature_names)\nimportances_std = pd.Series(result.importances_std, index=feature_names)\nsorted_importances = importances.sort_values(ascending=False)\n\n# Plot with matching error bars\nfig, ax = plt.subplots(figsize=(10, 6))\nsorted_importances.plot.bar(yerr=importances_std[sorted_importances.index], ax=ax)\nax.set_title(\"Permutation Importance (Validation Set)\")\nax.set_ylabel(\"Mean Accuracy Decrease\")\nfig.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:45:05.643171Z","iopub.execute_input":"2025-06-19T19:45:05.643608Z","iopub.status.idle":"2025-06-19T19:45:07.511933Z","shell.execute_reply.started":"2025-06-19T19:45:05.643577Z","shell.execute_reply":"2025-06-19T19:45:07.510665Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Great! We are now left with a model that benefits from each feature in its dataset. We can proceed to fine tuning the model.","metadata":{}},{"cell_type":"markdown","source":"## Fine Tuning: Minimizing False Negatives\n\nThe current model is chosen based on the hyperparameters that resulted in the best F-beta Score. However, the default threshold used to predict class that the RandomForestClassifier uses is 0.5. Below, we will compare the predictive performance of the trained model when we alter this threshold.","metadata":{}},{"cell_type":"markdown","source":"First things first, we need to retrain our model on the combination of the train and validation datasets. ","metadata":{}},{"cell_type":"code","source":"# Drop features (datsets do not leak since random_state kept constant)\nX = df_encoded.drop(columns=['SHORTNESS_OF_BREATH', 'ANXIETY', 'GENDER', 'SMOKING', 'FATIGUE', 'AGE', 'CHRONIC_DISEASE', 'LUNG_CANCER']).values\ny = df_encoded['LUNG_CANCER'].values\nfeature_names = df_encoded.drop(columns=['SHORTNESS_OF_BREATH', 'ANXIETY', 'GENDER', 'SMOKING', 'FATIGUE', 'AGE', 'CHRONIC_DISEASE', 'LUNG_CANCER']).columns\n\n# Split X, y into train and test splits and stratify based on target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle=True, stratify=y)\n\n# Create classifier with best hyperparameters\nclf = RandomForestClassifier(**grid2.best_params_)\n\n# Train\nclf.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T20:15:16.545013Z","iopub.execute_input":"2025-06-19T20:15:16.545424Z","iopub.status.idle":"2025-06-19T20:15:16.701405Z","shell.execute_reply.started":"2025-06-19T20:15:16.545397Z","shell.execute_reply":"2025-06-19T20:15:16.700550Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get predicted positive class probabilities\ny_score = clf.predict_proba(X_train)[:, 1]\n\n# Determine Precision-Recall scores\nprec, recall, thresholds = precision_recall_curve(y_train, y_score)\npr_display = PrecisionRecallDisplay(precision=prec, recall=recall)\n\n# Find threshold that minimizes False Negatives\nnum_false_negatives = []\ntn_list, fp_list, fn_list, tp_list = [], [], [], []\nfor threshold in thresholds: \n    y_pred_at_threshold = np.where(y_score >= threshold, 1, 0)\n    # indices_of_true_negatives = np.where(y_train == 1)[0]\n    # correct = y_pred_at_threshold[indices_of_true_negatives].sum()\n    # num_false_negatives.append(len(indices_of_true_negatives) - correct)\n\n    # Calculate true negatives, false positives, false negatives, true positives\n    tn, fp, fn, tp = confusion_matrix(y_train, y_pred_at_threshold).ravel().tolist()\n    tn_list.append(tn)\n    fp_list.append(fp)\n    fn_list.append(fn)\n    tp_list.append(tp)\n\n# # Threshold for best F-beta score\n# beta = 2\n# f_beta_score = ((1 + beta ** 2) * prec * recall) / ((beta ** 2 * prec) + recall) # as a function of precision and recall\n# idx = f_beta_score.argmax()\n# best_fb_threshold = thresholds[idx]\n# print('Best Threshold=%f, F-Score=%.3f' % (best_fb_threshold, f_beta_score[idx]))\n\n# # Confusion Matrix (default threshold)\n# y_pred = clf.predict(X_train)\n# cm = confusion_matrix(y_train, y_pred)\n# cm_display = ConfusionMatrixDisplay(cm)\n\n# # Confusion Matrix (threshold for best F-beta score)\n# y_pred_fb = np.where(y_score >= best_fb_threshold, 1, 0)\n# cm = confusion_matrix(y_train, y_pred_fb)\n# cm_display_fb = ConfusionMatrixDisplay(cm)\n\n# # Plot all together\n# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n# cm_display.plot(ax=ax1)\n# ax1.set_title('Default Threshold=0.5')\n# cm_display_fb.plot(ax=ax2)\n# ax2.set_title('Best F-beta Score Threshold=%f' % best_fb_threshold)\n# plt.show()\n\n# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n# pr_display.plot(ax=ax1)\n# default_ind = np.argwhere(thresholds == 0.5)\n# ax1.scatter(recall[default_ind], prec[default_ind], marker='o', c='r')\n# ax1.legend(['PRC', 'Default Threshold=0.5'])\n# pr_display.plot(ax=ax2)\n# ax2.scatter(recall[idx], prec[idx], marker='o', c='r')\n# ax2.legend(['PRC', 'Best F-beta Score Threshold=%f' % best_fb_threshold])\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T20:42:41.528084Z","iopub.execute_input":"2025-06-19T20:42:41.528827Z","iopub.status.idle":"2025-06-19T20:42:41.589401Z","shell.execute_reply.started":"2025-06-19T20:42:41.528796Z","shell.execute_reply":"2025-06-19T20:42:41.588576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(thresholds, fn_list, label='False Negatives')\nplt.plot(thresholds, fp_list, label='False Positives')\nplt.xlabel('Threshold')\nplt.ylabel('Count')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T20:44:32.834935Z","iopub.execute_input":"2025-06-19T20:44:32.835248Z","iopub.status.idle":"2025-06-19T20:44:33.020099Z","shell.execute_reply.started":"2025-06-19T20:44:32.835225Z","shell.execute_reply":"2025-06-19T20:44:33.018685Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Results and Insights","metadata":{"execution":{"iopub.status.busy":"2025-06-19T00:06:21.178965Z","iopub.execute_input":"2025-06-19T00:06:21.179612Z","iopub.status.idle":"2025-06-19T00:06:21.183783Z","shell.execute_reply.started":"2025-06-19T00:06:21.179585Z","shell.execute_reply":"2025-06-19T00:06:21.183009Z"}}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Conclusion and Next Steps\n\n\nThank you for checking out my notebook!","metadata":{}}]}